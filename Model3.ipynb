{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n6Gphul_UaJ7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "SPklz7pUU6uL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=2,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(256*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "WcSvGgK4U9PS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(BasicBlock, [5,1,1,4])"
      ],
      "metadata": {
        "id": "nQFJdK7vU-9A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to use\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate ResNet model\n",
        "model = model.to(device)\n",
        "\n",
        "# Generate example input data\n",
        "inputs = torch.randn(1, 3, 64, 64).to(device)\n",
        "\n",
        "# Print model summary\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB8pK1aOVAaw",
        "outputId": "ada58943-ba86-4e6c-f302-a2c9692c504c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 33, 33]             768\n",
            "       BatchNorm2d-2           [-1, 64, 33, 33]             128\n",
            "            Conv2d-3           [-1, 32, 33, 33]          18,432\n",
            "       BatchNorm2d-4           [-1, 32, 33, 33]              64\n",
            "            Conv2d-5           [-1, 32, 33, 33]           9,216\n",
            "       BatchNorm2d-6           [-1, 32, 33, 33]              64\n",
            "            Conv2d-7           [-1, 32, 33, 33]           2,048\n",
            "       BatchNorm2d-8           [-1, 32, 33, 33]              64\n",
            "        BasicBlock-9           [-1, 32, 33, 33]               0\n",
            "           Conv2d-10           [-1, 32, 33, 33]           9,216\n",
            "      BatchNorm2d-11           [-1, 32, 33, 33]              64\n",
            "           Conv2d-12           [-1, 32, 33, 33]           9,216\n",
            "      BatchNorm2d-13           [-1, 32, 33, 33]              64\n",
            "       BasicBlock-14           [-1, 32, 33, 33]               0\n",
            "           Conv2d-15           [-1, 32, 33, 33]           9,216\n",
            "      BatchNorm2d-16           [-1, 32, 33, 33]              64\n",
            "           Conv2d-17           [-1, 32, 33, 33]           9,216\n",
            "      BatchNorm2d-18           [-1, 32, 33, 33]              64\n",
            "       BasicBlock-19           [-1, 32, 33, 33]               0\n",
            "           Conv2d-20           [-1, 32, 33, 33]           9,216\n",
            "      BatchNorm2d-21           [-1, 32, 33, 33]              64\n",
            "           Conv2d-22           [-1, 32, 33, 33]           9,216\n",
            "      BatchNorm2d-23           [-1, 32, 33, 33]              64\n",
            "       BasicBlock-24           [-1, 32, 33, 33]               0\n",
            "           Conv2d-25           [-1, 32, 33, 33]           9,216\n",
            "      BatchNorm2d-26           [-1, 32, 33, 33]              64\n",
            "           Conv2d-27           [-1, 32, 33, 33]           9,216\n",
            "      BatchNorm2d-28           [-1, 32, 33, 33]              64\n",
            "       BasicBlock-29           [-1, 32, 33, 33]               0\n",
            "           Conv2d-30           [-1, 64, 17, 17]          18,432\n",
            "      BatchNorm2d-31           [-1, 64, 17, 17]             128\n",
            "           Conv2d-32           [-1, 64, 17, 17]          36,864\n",
            "      BatchNorm2d-33           [-1, 64, 17, 17]             128\n",
            "           Conv2d-34           [-1, 64, 17, 17]           2,048\n",
            "      BatchNorm2d-35           [-1, 64, 17, 17]             128\n",
            "       BasicBlock-36           [-1, 64, 17, 17]               0\n",
            "           Conv2d-37            [-1, 128, 9, 9]          73,728\n",
            "      BatchNorm2d-38            [-1, 128, 9, 9]             256\n",
            "           Conv2d-39            [-1, 128, 9, 9]         147,456\n",
            "      BatchNorm2d-40            [-1, 128, 9, 9]             256\n",
            "           Conv2d-41            [-1, 128, 9, 9]           8,192\n",
            "      BatchNorm2d-42            [-1, 128, 9, 9]             256\n",
            "       BasicBlock-43            [-1, 128, 9, 9]               0\n",
            "           Conv2d-44            [-1, 256, 5, 5]         294,912\n",
            "      BatchNorm2d-45            [-1, 256, 5, 5]             512\n",
            "           Conv2d-46            [-1, 256, 5, 5]         589,824\n",
            "      BatchNorm2d-47            [-1, 256, 5, 5]             512\n",
            "           Conv2d-48            [-1, 256, 5, 5]          32,768\n",
            "      BatchNorm2d-49            [-1, 256, 5, 5]             512\n",
            "       BasicBlock-50            [-1, 256, 5, 5]               0\n",
            "           Conv2d-51            [-1, 256, 5, 5]         589,824\n",
            "      BatchNorm2d-52            [-1, 256, 5, 5]             512\n",
            "           Conv2d-53            [-1, 256, 5, 5]         589,824\n",
            "      BatchNorm2d-54            [-1, 256, 5, 5]             512\n",
            "       BasicBlock-55            [-1, 256, 5, 5]               0\n",
            "           Conv2d-56            [-1, 256, 5, 5]         589,824\n",
            "      BatchNorm2d-57            [-1, 256, 5, 5]             512\n",
            "           Conv2d-58            [-1, 256, 5, 5]         589,824\n",
            "      BatchNorm2d-59            [-1, 256, 5, 5]             512\n",
            "       BasicBlock-60            [-1, 256, 5, 5]               0\n",
            "           Conv2d-61            [-1, 256, 5, 5]         589,824\n",
            "      BatchNorm2d-62            [-1, 256, 5, 5]             512\n",
            "           Conv2d-63            [-1, 256, 5, 5]         589,824\n",
            "      BatchNorm2d-64            [-1, 256, 5, 5]             512\n",
            "       BasicBlock-65            [-1, 256, 5, 5]               0\n",
            "           Linear-66                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 4,856,522\n",
            "Trainable params: 4,856,522\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 10.86\n",
            "Params size (MB): 18.53\n",
            "Estimated Total Size (MB): 29.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define data augmentation transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                         std=[0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                         std=[0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "# load CIFAR10 train and test datasets\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEETQq-ZWCpg",
        "outputId": "314b64a6-d60a-4222-8b7b-52758c62f827"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12987730.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "xIfeJzaoWlnB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "CE8R3CWYWhP-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.RandomCrop(32, padding=4),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjHqMoelWjFK",
        "outputId": "eb33bfbe-da6b-4066-c9a9-ca644b3e0fb7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        log_interval = 100\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() * data.size(0)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxMEp8viWvzI",
        "outputId": "89b4f424-d4a2-4013-9556-e1effa544670"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.808073\n",
            "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.480679\n",
            "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 1.378804\n",
            "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.366213\n",
            "Test set: Average loss: 1.2334, Accuracy: 5608/10000 (56%)\n",
            "\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.066705\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.004192\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.865692\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.872037\n",
            "Test set: Average loss: 0.9721, Accuracy: 6668/10000 (67%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.734947\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.871761\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.732248\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.819324\n",
            "Test set: Average loss: 0.8295, Accuracy: 7062/10000 (71%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.560251\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.553902\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.533274\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.514162\n",
            "Test set: Average loss: 0.7675, Accuracy: 7340/10000 (73%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.608053\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.595611\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.556079\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.577831\n",
            "Test set: Average loss: 0.6774, Accuracy: 7661/10000 (77%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.497986\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.580990\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.581116\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.642396\n",
            "Test set: Average loss: 0.6331, Accuracy: 7798/10000 (78%)\n",
            "\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.507212\n",
            "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.602603\n",
            "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.429410\n",
            "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.390254\n",
            "Test set: Average loss: 0.5977, Accuracy: 7993/10000 (80%)\n",
            "\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.405270\n",
            "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.485967\n",
            "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.524515\n",
            "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.456602\n",
            "Test set: Average loss: 0.5562, Accuracy: 8146/10000 (81%)\n",
            "\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.407532\n",
            "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.335513\n",
            "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.531573\n",
            "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.373889\n",
            "Test set: Average loss: 0.5408, Accuracy: 8136/10000 (81%)\n",
            "\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.337766\n",
            "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.581968\n",
            "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.500799\n",
            "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.413390\n",
            "Test set: Average loss: 0.5286, Accuracy: 8255/10000 (83%)\n",
            "\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.267875\n",
            "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.364277\n",
            "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.357838\n",
            "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.354294\n",
            "Test set: Average loss: 0.4746, Accuracy: 8345/10000 (83%)\n",
            "\n",
            "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.386076\n",
            "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 0.231992\n",
            "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.218618\n",
            "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 0.427901\n",
            "Test set: Average loss: 0.5304, Accuracy: 8232/10000 (82%)\n",
            "\n",
            "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.369084\n",
            "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 0.274805\n",
            "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.279995\n",
            "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 0.252162\n",
            "Test set: Average loss: 0.4734, Accuracy: 8443/10000 (84%)\n",
            "\n",
            "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.345856\n",
            "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 0.233085\n",
            "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.225674\n",
            "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 0.297147\n",
            "Test set: Average loss: 0.4622, Accuracy: 8496/10000 (85%)\n",
            "\n",
            "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.171635\n",
            "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 0.191434\n",
            "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.169586\n",
            "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 0.293743\n",
            "Test set: Average loss: 0.4270, Accuracy: 8567/10000 (86%)\n",
            "\n",
            "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.218492\n",
            "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 0.171578\n",
            "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.317716\n",
            "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 0.256454\n",
            "Test set: Average loss: 0.4195, Accuracy: 8628/10000 (86%)\n",
            "\n",
            "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.233461\n",
            "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 0.184981\n",
            "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.289810\n",
            "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 0.298912\n",
            "Test set: Average loss: 0.4082, Accuracy: 8616/10000 (86%)\n",
            "\n",
            "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.206770\n",
            "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 0.428909\n",
            "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.210029\n",
            "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 0.201517\n",
            "Test set: Average loss: 0.3950, Accuracy: 8664/10000 (87%)\n",
            "\n",
            "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.193166\n",
            "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 0.224477\n",
            "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.301894\n",
            "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 0.251213\n",
            "Test set: Average loss: 0.3884, Accuracy: 8711/10000 (87%)\n",
            "\n",
            "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.225216\n",
            "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 0.192823\n",
            "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.253642\n",
            "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 0.187154\n",
            "Test set: Average loss: 0.3996, Accuracy: 8717/10000 (87%)\n",
            "\n",
            "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.206084\n",
            "Train Epoch: 20 [12800/50000 (26%)]\tLoss: 0.180852\n",
            "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.196220\n",
            "Train Epoch: 20 [38400/50000 (77%)]\tLoss: 0.307764\n",
            "Test set: Average loss: 0.4097, Accuracy: 8693/10000 (87%)\n",
            "\n",
            "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.141440\n",
            "Train Epoch: 21 [12800/50000 (26%)]\tLoss: 0.151917\n",
            "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.280744\n",
            "Train Epoch: 21 [38400/50000 (77%)]\tLoss: 0.257514\n",
            "Test set: Average loss: 0.3771, Accuracy: 8783/10000 (88%)\n",
            "\n",
            "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.157113\n",
            "Train Epoch: 22 [12800/50000 (26%)]\tLoss: 0.305345\n",
            "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.283761\n",
            "Train Epoch: 22 [38400/50000 (77%)]\tLoss: 0.252474\n",
            "Test set: Average loss: 0.4292, Accuracy: 8688/10000 (87%)\n",
            "\n",
            "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.226668\n",
            "Train Epoch: 23 [12800/50000 (26%)]\tLoss: 0.161180\n",
            "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.106668\n",
            "Train Epoch: 23 [38400/50000 (77%)]\tLoss: 0.125907\n",
            "Test set: Average loss: 0.4383, Accuracy: 8657/10000 (87%)\n",
            "\n",
            "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.134707\n",
            "Train Epoch: 24 [12800/50000 (26%)]\tLoss: 0.180211\n",
            "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.220492\n",
            "Train Epoch: 24 [38400/50000 (77%)]\tLoss: 0.236537\n",
            "Test set: Average loss: 0.3523, Accuracy: 8874/10000 (89%)\n",
            "\n",
            "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.122384\n",
            "Train Epoch: 25 [12800/50000 (26%)]\tLoss: 0.117228\n",
            "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.169509\n",
            "Train Epoch: 25 [38400/50000 (77%)]\tLoss: 0.215157\n",
            "Test set: Average loss: 0.3700, Accuracy: 8924/10000 (89%)\n",
            "\n",
            "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.213567\n",
            "Train Epoch: 26 [12800/50000 (26%)]\tLoss: 0.275977\n",
            "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.118848\n",
            "Train Epoch: 26 [38400/50000 (77%)]\tLoss: 0.233747\n",
            "Test set: Average loss: 0.3720, Accuracy: 8886/10000 (89%)\n",
            "\n",
            "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.137866\n",
            "Train Epoch: 27 [12800/50000 (26%)]\tLoss: 0.105750\n",
            "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.176930\n",
            "Train Epoch: 27 [38400/50000 (77%)]\tLoss: 0.150926\n",
            "Test set: Average loss: 0.4516, Accuracy: 8686/10000 (87%)\n",
            "\n",
            "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.060374\n",
            "Train Epoch: 28 [12800/50000 (26%)]\tLoss: 0.083783\n",
            "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.112941\n",
            "Train Epoch: 28 [38400/50000 (77%)]\tLoss: 0.131414\n",
            "Test set: Average loss: 0.3914, Accuracy: 8806/10000 (88%)\n",
            "\n",
            "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.090145\n",
            "Train Epoch: 29 [12800/50000 (26%)]\tLoss: 0.121636\n",
            "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.118859\n",
            "Train Epoch: 29 [38400/50000 (77%)]\tLoss: 0.050344\n",
            "Test set: Average loss: 0.4108, Accuracy: 8793/10000 (88%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}